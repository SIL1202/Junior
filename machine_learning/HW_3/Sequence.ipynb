{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T15:44:06.065599Z",
     "iopub.status.busy": "2026-01-07T15:44:06.064658Z",
     "iopub.status.idle": "2026-01-07T15:44:06.299987Z",
     "shell.execute_reply": "2026-01-07T15:44:06.299145Z",
     "shell.execute_reply.started": "2026-01-07T15:44:06.065568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Head:\n",
      "                                           sequences      labels  num_samples  \\\n",
      "0  [141, 35, 28, 17, 15, 3, 83, 71, 1, 83, 302, 1...  Category_D         4000   \n",
      "1  [51, 35, 28, 17, 15, 3, 180, 28, 17, 15, 3, 18...  Category_A         4000   \n",
      "2  [141, 35, 28, 17, 15, 3, 69, 85, 75, 1, 73, 85...  Category_E         4000   \n",
      "3  [50, 69, 85, 75, 51, 35, 28, 17, 15, 3, 180, 2...  Category_E         4000   \n",
      "4  [50, 200, 225, 158, 51, 35, 28, 17, 15, 3, 201...  Category_C         4000   \n",
      "\n",
      "   vocab_size  \n",
      "0       10000  \n",
      "1       10000  \n",
      "2       10000  \n",
      "3       10000  \n",
      "4       10000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Set path (based on your screenshot)\n",
    "base_path = '/kaggle/input/sequence-recognition-using-rnn-lstm-gru/'\n",
    "\n",
    "# Read JSON file\n",
    "# Note: If JSON is one object per line, add lines=True\n",
    "try:\n",
    "    # Attempt to read the JSON file using pandas\n",
    "    train_df = pd.read_json(f'{base_path}train.json')\n",
    "    print(\"Train Data Head:\")\n",
    "    print(train_df.head())\n",
    "except:\n",
    "    # If the above fails, it might be standard JSON format, so use json package directly\n",
    "    with open(f'{base_path}train.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Print keys of the first element if it's a list, otherwise print keys of the dictionary\n",
    "    print(\"Data keys:\", data[0].keys() if isinstance(data, list) else data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T15:49:24.959737Z",
     "iopub.status.busy": "2026-01-07T15:49:24.959416Z",
     "iopub.status.idle": "2026-01-07T15:49:30.606723Z",
     "shell.execute_reply": "2026-01-07T15:49:30.605749Z",
     "shell.execute_reply.started": "2026-01-07T15:49:24.959715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape: torch.Size([4000, 10169])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# 1. Label Encoding\n",
    "le = LabelEncoder()\n",
    "# Transform labels to numerical indices\n",
    "train_df['label_idx'] = le.fit_transform(train_df['labels'])\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# 2. Convert Sequences to Tensor and perform Padding\n",
    "# When sequence lengths vary, pad with 0 to make every data entry the same length\n",
    "sequences = [torch.tensor(s) for s in train_df['sequences']]\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "\n",
    "print(f\"Padded shape: {padded_sequences.shape}\") # (Number of samples, Max length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T15:50:45.408048Z",
     "iopub.status.busy": "2026-01-07T15:50:45.407109Z",
     "iopub.status.idle": "2026-01-07T15:50:45.449976Z",
     "shell.execute_reply": "2026-01-07T15:50:45.449144Z",
     "shell.execute_reply.started": "2026-01-07T15:50:45.408014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # Embedding layer: Convert 10000 IDs to continuous vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Classifier: Since it's bidirectional LSTM, hidden_size is multiplied by 2\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, Seq_Len)\n",
    "        x = self.embedding(x)  # Convert to (Batch, Seq_Len, Embedding_Dim)\n",
    "        \n",
    "        # out contains outputs for all time steps, (h_n, c_n) is the state at the last time step\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Concatenate the last hidden state of both directions\n",
    "        cat_hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
    "        \n",
    "        return self.fc(cat_hidden)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMClassifier(vocab_size=10000, embedding_dim=128, hidden_size=64, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T15:51:04.749967Z",
     "iopub.status.busy": "2026-01-07T15:51:04.748636Z",
     "iopub.status.idle": "2026-01-07T15:51:08.447607Z",
     "shell.execute_reply": "2026-01-07T15:51:08.446668Z",
     "shell.execute_reply.started": "2026-01-07T15:51:04.749926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Simple training demonstration\n",
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    for seqs, targets in loader:\n",
    "        seqs, targets = seqs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(seqs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:08:34.124456Z",
     "iopub.status.busy": "2026-01-07T16:08:34.123742Z",
     "iopub.status.idle": "2026-01-07T16:13:54.738709Z",
     "shell.execute_reply": "2026-01-07T16:13:54.737754Z",
     "shell.execute_reply.started": "2026-01-07T16:08:34.124427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/10, Loss: 0.6233\n",
      "Epoch 2/10, Loss: 0.0308\n",
      "Epoch 3/10, Loss: 0.0061\n",
      "Epoch 4/10, Loss: 0.0045\n",
      "Epoch 5/10, Loss: 0.0034\n",
      "Epoch 6/10, Loss: 0.0037\n",
      "Epoch 7/10, Loss: 0.0034\n",
      "Epoch 8/10, Loss: 0.0029\n",
      "Epoch 9/10, Loss: 0.0025\n",
      "Epoch 10/10, Loss: 0.0023\n",
      "Submission saved! 趕快去 Output 區下載並上傳吧！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Parameter Settings ---\n",
    "MAX_LEN = 128      # Limit sequence length to avoid memory explosion\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- 1. Data Processing ---\n",
    "# Read data\n",
    "base_path = '/kaggle/input/sequence-recognition-using-rnn-lstm-gru/'\n",
    "train_df = pd.read_json(f'{base_path}train.json')\n",
    "test_df = pd.read_json(f'{base_path}test.json')\n",
    "\n",
    "# Label Encoding (Category_A -> 0)\n",
    "le = LabelEncoder()\n",
    "train_df['label_idx'] = le.fit_transform(train_df['labels'])\n",
    "num_classes = len(le.classes_)\n",
    "vocab_size = 10000 + 1 # Considering 0 as Padding\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, sequences, labels=None):\n",
    "        # Limit length and convert to Tensor\n",
    "        self.sequences = [torch.tensor(s[:MAX_LEN]) for s in sequences]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.sequences[idx], self.labels[idx]\n",
    "        return self.sequences[idx]\n",
    "\n",
    "# Custom collate_fn for dynamic Padding\n",
    "def collate_fn(batch):\n",
    "    if isinstance(batch[0], tuple): # Training mode\n",
    "        seqs, labels = zip(*batch)\n",
    "        seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "        return seqs_padded, torch.tensor(labels)\n",
    "    else: # Test mode\n",
    "        seqs_padded = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "        return seqs_padded\n",
    "\n",
    "train_loader = DataLoader(SeqDataset(train_df['sequences'], train_df['label_idx']), \n",
    "                          batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# --- 2. Model Definition (Using Bidirectional GRU, more efficient than LSTM) ---\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        # _ is the hidden state of the last item\n",
    "        _, hn = self.gru(embedded) \n",
    "        # Get the last hidden state of both directions and concatenate\n",
    "        cat_hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
    "        return self.fc(cat_hn)\n",
    "\n",
    "model = GRUClassifier(vocab_size, EMBED_DIM, HIDDEN_DIM, num_classes).to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- 3. Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        \n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# --- 4. Prediction and Submission Generation ---\n",
    "model.eval()\n",
    "test_loader = DataLoader(SeqDataset(test_df['sequences']), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch in test_loader:\n",
    "        x_batch = x_batch.to(DEVICE)\n",
    "        outputs = model(x_batch)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Convert back to original label strings\n",
    "final_labels = le.inverse_transform(predictions)\n",
    "\n",
    "# Correction: If test_df does not have 'id' column, use index as id\n",
    "if 'id' in test_df.columns:\n",
    "    test_ids = test_df['id']\n",
    "else:\n",
    "    # According to sample_submission.csv format, typically an index starting from 0 or 1\n",
    "    test_ids = test_df.index \n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids, \n",
    "    'labels': final_labels\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved! Go to Output area to download and upload!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14738261,
     "sourceId": 125099,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31239,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
