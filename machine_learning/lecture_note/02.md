#### åƒæ•¸è¶Šå¤šï¼Œè¨“ç·´æ™‚è§€å¯Ÿåˆ°çš„ Loss è·Ÿæ¸¬è©¦æ™‚è§€å¯Ÿåˆ°çš„ Loss æœƒå·®è·è¶Šå¤§

> More parameters, easier to overfit.

#### Pokemon/ Digimon Classifier 

â€‹	We want to find a function . . .

â€‹	Determine a function with unknown parameters ( based on domain knowledge )

â€‹	$f_h$ : function with threshold $h$

â€‹	æ‰€æœ‰ $h$ é›†åˆèµ·ä¾†å«åš $H$ = { 1, 2, $\cdots$ , 10,000 }

â€‹	$|H|$: æœ‰å¤šå°‘å¯èƒ½çš„é¸æ“‡ï¼Œç¨±ä½œ model "complexity"

â€‹	å®šç¾©å®Œæœ‰æœªçŸ¥æ•¸çš„ function å¾Œï¼Œå†ä¾†å®šç¾© $Loss$ 

â€‹	$Loss$ function (given data)
$$
D = \{(x^1, \hat{y}^1), (x^2, \hat{y}^2), \cdots , (x^N, \hat{y}^N)\}
$$
â€‹	Loss of a threshold $h$ given data set $D$

#### 	Error rate : 

$$
L(h, D) = \frac{1}{N}\sum\limits_{n=1}^Nl(h, x^n, \hat{y}^n)	
$$

$$
\text{if} f_h(x^n) \neq \hat{y}^n \\
â€‹		&\text{output 1}\\
â€‹	\text{otherwise }\\
â€‹		&\text{output 0}\\
$$

---

#### Training Examples

â€‹	ç†æƒ³ï¼š$h^{all} = arg min L(h, D_{all})$

â€‹	ç¾å¯¦ï¼š$h^{train} = arg minL(h, D_{train})$

We hope $L(h^{train}, D_{all})$ and $L(h^{all}, D_{all})$ are close.

---

#### What do we want

- ç›®æ¨™ï¼š
  - $L((D_{train}), D_{all}) - L(h_{all}, D_{all}) â‰¤ Î´$
  - â†’ Generalization gap (æ³›åŒ–èª¤å·®) å°æ–¼ $Î´$ã€‚
- æ¢ä»¶ï¼š
  - åªè¦æ»¿è¶³ï¼š$âˆ€ h âˆˆ ğ“—$ (Hypothesis set, å‡è¨­é›†åˆ)ï¼Œ$|L(h, (D_{train})) âˆ’ L(h, D_{all})| â‰¤ Îµ (Îµ = Î´/2)$
  - â†’ å°æ‰€æœ‰å¯èƒ½æ¨¡å‹ $h$ï¼Œè¨“ç·´è³‡æ–™ $(D_{train})$ å’Œå…¨éƒ¨è³‡æ–™ $(D_{all})$ çš„ $Loss$ å·®è·ä¸è¶…é $Îµ$ã€‚

##### æ¨å°éç¨‹ï¼š

- $L((h_{train}), D_all) â‰¤ L(h_{train}, (D_{train})) + Î´/2$
- $L(h_{train}, (D_{train})) â‰¤ L(h_all, (D_{train}))$ï¼ˆå› ç‚º $h_{train}$ æ˜¯ $arg min L(h, (D_{train}))$ï¼‰
- $L(h_all, (D_{train})) â‰¤ L(h_{all}, D_{all}) + Î´/2$
- åˆèµ·ä¾†ï¼š$L(h_{train}, D_{all}) âˆ’ L(h_{all}, D_{all}) â‰¤ Î´$

---

#### å’Œçš„ç†è§£ (Union Bound)

- $P(D_{train}$ is bad$)=â‹ƒhâˆˆHP$($D_{train}$ is bad due to $h$)
- $â‰¤âˆ‘hâˆˆHP(D_{train}$ is bad due to $h)$
- $â‰¤âˆ£Hâˆ£Ã—2expâ¡(âˆ’2NÎµ2)$
- **è¦–è¦ºåœ–ç¤º**ï¼š
  - æ¯å€‹ h å°æ‡‰ä¸€ç¾¤å¯èƒ½ç™¼ç”Ÿ bad çš„æƒ…æ³ï¼ˆå¦‚æ©˜è‰²é»ï¼‰ï¼Œç”¨ä¸åŒæ¡†æ¡†ä»£è¡¨ä¸åŒ hã€‚
  - é›–ç„¶å¯¦éš›ä¸Šæ˜¯è¯é›† (Union)ï¼Œä½†ç‚ºäº†æ–¹ä¾¿ä¸Šç•Œä¼°è¨ˆï¼Œç”¨åŠ ç¸½ (Sum) ä¾†è¨ˆç®—ã€‚é€™æ˜¯ Union Bound çš„æ¦‚å¿µã€‚

---

#### Hoeffding's Inequality (éœå¤«ä¸ä¸ç­‰å¼) æ‡‰ç”¨

- **å…¬å¼**ï¼š
  - $P(D_{train}$ is bad due to $h)â‰¤2expâ¡(âˆ’2NÎµ2)$
- **è£œå……**ï¼š
  - Loss L ç¯„åœæ˜¯ [0,1]ã€‚
  - N æ˜¯ $D_{train}$ è£¡é¢çš„æ¨£æœ¬æ•¸ (Number of training examples)ã€‚

---

#### å¦‚ä½•è®“ P($D_{train}$ is bad) è®Šå°ï¼Ÿ

- **å…©å€‹æ–¹å‘**ï¼š
  - å¢åŠ  Nï¼ˆæ¨£æœ¬æ•¸ Number of training examplesï¼‰
  - æ¸›å°‘ $âˆ£Hâˆ£$ï¼ˆæ¨¡å‹è¤‡é›œåº¦ Model complexityï¼‰
- **ä½†è¦æ³¨æ„**ï¼š
  - $âˆ£Hâˆ£$ ä¸èƒ½éš¨ä¾¿ç¸®å°ï¼Œå› ç‚ºå¯èƒ½æœƒå°è‡´æ¨¡å‹ç„¡æ³• fit è³‡æ–™ â†’ ç†æƒ³å´©å£ (Ideal collapse)ã€‚

---

#### æ¨¡å‹è¤‡é›œåº¦èˆ‡æ¨£æœ¬æ•¸çš„ Tradeoff (å–æ¨)

- Larger N å’Œ Smaller $âˆ£Hâˆ£$ â†’ $L(h_{train},D_{all})âˆ’L(h_{all},D_{all})â‰¤Î´$
- Smaller $âˆ£Hâˆ£$ â†’ Larger $L(h_{all},D_{all})$
- åœ–ç¤ºè§£é‡‹ï¼š
  - $âˆ£Hâˆ£$ å¤§æ™‚ï¼Œ$h_{all}$ å¯ä»¥æ‰¾åˆ°æ›´å°çš„ Lossã€‚
  - $âˆ£Hâˆ£$ å°æ™‚ï¼Œé¸æ“‡æ¯”è¼ƒå°‘ï¼Œ$h_{all}$ çš„ Loss å¯èƒ½æ¯”è¼ƒå¤§ã€‚

---

#### ç¯„ä¾‹è¨ˆç®— (Example Calculation)

- $âˆ£Hâˆ£$=10,000ï¼Œ$Î´$=0.1ï¼Œ$Îµ$=0.1
- $Nâ‰¥\frac{logâ¡(2Ã—\frac{âˆ£H|}{Î´})}{2Îµ^2}$
- å¯¦éš›æ•¸å­—ï¼š
  - $âˆ£Hâˆ£$=10,000ï¼ŒN=100ï¼Œ$P(D_{train}$ is bad$) \leq$2707 â†’ å¾ˆç³Ÿã€‚
  - $âˆ£Hâˆ£$=10,000ï¼ŒN=500ï¼Œ$P(D_{train}$ is bad$)â‰¤$0.91 â†’ ä»ç„¶å¤ªå¤§ã€‚
  - $âˆ£Hâˆ£$=10,000ï¼ŒN=1000ï¼Œ$P(D_{train}$ is bad$)â‰¤$0.00004 â†’ å¯æ¥å—ã€‚

----

#### æ ¸å¿ƒçµè«– (Summary)

- **æ¨¡å‹è¶Šè¤‡é›œ (Larger Model Complexity $âˆ£Hâˆ£$)** â†’ è¶Šå®¹æ˜“ overfit â†’ éœ€è¦æ›´å¤šè³‡æ–™ (Larger N)ã€‚
- **è¦å¹³è¡¡æ¨¡å‹è¤‡é›œåº¦èˆ‡è³‡æ–™é‡**ï¼š
  - ä¸å¸Œæœ›æ¨¡å‹å¤ªç°¡å–® (Too simple)ï¼Œä¹Ÿä¸å¸Œæœ›å¤ªè¤‡é›œ (Too complex) è€Œè³‡æ–™é‡ä¸è¶³ (Insufficient data)ã€‚