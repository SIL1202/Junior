## 機器學習筆記：模型複雜度與訓練樣本數的關係（純 LaTeX 版本）

### Overfitting（過擬合）定義

- 當模型在訓練資料上表現良好，但在未見過的新資料（測試資料）上表現變差：

  $L(h,D_{train})≪L(h,D_{test})$

- 因為模型太複雜，記住了訓練資料的細節（甚至是雜訊），而非學到真正的規律。

### Deep vs. Shallow Network 比較

#### Deep Network（深層模型）

- 多層（K 層）
- 每層單位較少（例如 2 neurons）
- Hypothesis set $∣H∣$ 較小 → 不易 overfitting
- 可以用較少參數達成複雜表現力
- 比較適合學習階層式特徵（Representation Learning）

#### Shallow Network（淺層模型）

- 層數少（1 層）
- 每層單位很多（例如 2K 個 neurons）
- Hypothesis set $∣H∣$ 較大 → 容易 overfitting
- 雖然理論上也能近似複雜函數，但需要非常大的模型規模

#### 結論：

深度學習是「魚與熊掌兼得」的折衷：

- 同時兼顧模型表現力與較低的 overfitting 風險。
- 增加層數比單純增加單層寬度更有效率。