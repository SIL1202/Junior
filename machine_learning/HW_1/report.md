# Extreme Optimization of Non-linear Function Approximation
### 1. Detailed descriptions about the model you have designed
The final model is a Residual Deep Multi-Layer Perceptron (ResDeepMLP), specifically engineered to handle high-frequency periodic data.
- **Feature Integration Layer**: Instead of feeding raw coordinates, the model takes a 6–10 dimensional feature vector that combines polar coordinates (r, θ), KNN-based local context, target encoding, and symbolic features generated by PySR.

- **Residual Architecture**: The main network is built from several residual blocks. Each block has fully connected layers with skip connections, which helps gradients flow more easily and reduces vanishing-gradient issues as the network gets deeper.

- **Normalization & Activation**: I use Layer Normalization instead of BatchNorm to keep training stable with a small batch size (8). For the activation function, I use SiLU (Swish), which has a smooth, non‑monotonic gradient and tends to work well for detailed regression tasks.

- **SGDR Training Engine**: Training is done with Stochastic Gradient Descent with Warm Restarts (SGDR). The learning rate follows a “pulse‑like” schedule: it periodically jumps up to help the model escape local minima, then slowly decays again for fine‑grained adjustments

### 2. The novelty of your design
The novelty lies in the Hybrid Symbolic-Statistical-Neural approach:

- **Geometric-Domain Awareness**: Unlike standard black-box MLPs, our model "understands" the circular nature of the data through automated polar transformations and Fourier Feature Projections.

- **Symbolic Feature Injection**: I used PySR (Symbolic Regression) to "distill" the underlying physical laws into a mathematical formula, then fed that formula back into the neural network as a high-level guidance feature.

- **Dynamic Peak Decay**: I implemented a custom learning rate schedule where the "restart" peaks of the SGDR are reduced by 20% in each cycle. This mimics the human process of "coarse-to-fine" polishing, ensuring the model explores broadly at first and focuses narrowly at the end.

### 3. What impressive results you have achieved
- Error Reduction: We successfully reduced the Mean Squared Error (MSE) from an initial baseline of 0.0178 (raw MLP) to a high-precision regime of 0.00x.

- Convergence Speed: By introducing the KNN and Target Encoding priors, the model achieved in 50 epochs what the original model could not achieve in 500.

- Fit Quality: The final 5-Fold Ensemble produced a "Predicted vs. Actual" scatter plot that aligns almost perfectly along a 45-degree diagonal, indicating a near-perfect reconstruction of the unknown 3D surface.

![Screenshot 2026-01-10 at 5.03.05 AM](https://hackmd.io/_uploads/rkpM611Sbg.png)

![Screenshot 2026-01-10 at 5.04.29 AM](https://hackmd.io/_uploads/HJgX0y1r-l.png)

### 4. The difficulty you encountered during the work
- **Spectral Bias**: Early in the project, the model suffered from the "F-Principle," where neural networks naturally learn low-frequency components but ignore high-frequency oscillations. This caused the Loss to plateau early.

- **Batch Instability**: When moving to very small Batch Sizes (n=8) to increase update frequency, the model became unstable. We solved this by switching to Layer Normalization and fine-tuning the weight decay.

- **Feature Dimension Mismatch**: A significant logic error occurred with r_bin dimensions (2D vs 1D), which caused the Target Encoding to fail silently. Diagnosing this required a deep dive into the data-type pipeline to ensure the "map" function was actually receiving valid indices.
![Screenshot 2026-01-10 at 5.09.47 AM](https://hackmd.io/_uploads/r1zaRyyrWg.png)

### 5. What have you learned from the work
- **Features > Architecture**: This project proved that Feature Engineering is the ceiling, while the model architecture is merely the tool to reach it. A simple polar transformation was more effective than adding ten hidden layers.

- **Optimization is a Rhythm**: Learning that a learning rate shouldn't just go down—it should "bounce" (SGDR)—was a revelation. It taught us that "shaking" the model is sometimes the only way to find the global optimum.

- **The Power of Ensembling**: Using K-Fold Cross-Validation taught us how to build a "committee of experts." Averaging five models not only lowered the error but also removed the volatility inherent in random weight initialization.